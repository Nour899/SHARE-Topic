{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ef7df40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_scatter\n",
    "import scanpy as sc\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6e94f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sc_to_tensor(sc_atac,sc_rna,path=\"\"):\n",
    "    \n",
    " n = sc_atac.X.nonzero()[0].shape[0]\n",
    " atac = torch.zeros([2,n])\n",
    " atac[0,:] = torch.from_numpy(sc_atac.X.nonzero()[0])\n",
    " atac[1,:] = torch.from_numpy(sc_atac.X.nonzero()[1])\n",
    " atac = atac.type(torch.LongTensor)\n",
    "\n",
    " rna = sc_rna.X.toarray()\n",
    " rna = torch.tensor(rna)\n",
    " return rna, atac\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a736e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cell_batches (atac,n_batches,batch_size,n_cells,device):\n",
    "    ## this function is used to create batch of cells for the atac data \n",
    "    # that should be transfered one at a time to the GPU to avoid GPU running \n",
    "     # out of memory. Because of the atac data format used to feed share-topic this function\n",
    "       # is necessary to find the coordinates of the non-zero reads per batch.\n",
    " c=torch.arange(0,n_batches,batch_size)\n",
    " c=torch.hstack((c,torch.tensor(n_cells)))\n",
    " \n",
    "    \n",
    " a,b,rep_c=torch.unique(atac[0,:],return_inverse =True,return_counts=True)\n",
    " rep_c=rep_c.type(torch.LongTensor) \n",
    " ren=torch.arange(n_cells)\n",
    " rep_c_=torch.repeat_interleave(ren, rep_c,dim=0)\n",
    " \n",
    "\n",
    " q=1\n",
    " t=0\n",
    " t_=0\n",
    "    \n",
    " atac_cell_batches=torch.zeros(c.shape[0],dtype=torch.int64)   \n",
    " for i in  torch.arange(batch_size,n_batches,batch_size):\n",
    "    atac_cell_batches[q]=t+torch.sum(rep_c[t_:i])\n",
    "    t=int(atac_cell_batches[q].item())\n",
    "    t_=i\n",
    "    q+=1\n",
    "    \n",
    " atac_cell_batches[q]=t+torch.sum(rep_c[t_:])\n",
    " \n",
    " return atac_cell_batches, c, rep_c,rep_c_, ren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e5f36b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_region_batches(atac,atac_cell_batches,ren):\n",
    " ###This function take the regions presented per batch of cells in the atac data with share-topic format. The presented regions\n",
    "    # are ord\n",
    " indices=torch.zeros([atac.shape[1]])\n",
    " indices2=torch.zeros([atac.shape[1]])\n",
    " regions=torch.tensor([])\n",
    " region_rep=torch.tensor([])\n",
    " region_rep_=torch.tensor([])\n",
    " region_batching=torch.zeros([atac_cell_batches.shape[0]])\n",
    " for i in torch.arange(1,atac_cell_batches.shape[0]):\n",
    "    \n",
    "   sorted, indices[int(atac_cell_batches[i-1].item()):\n",
    "                   int(atac_cell_batches[i].item())]= torch.sort(\n",
    "       atac[1,int(atac_cell_batches[i-1].item()):\n",
    "                 int(atac_cell_batches[i].item())])\n",
    "   sorted2,indices2[int(atac_cell_batches[i-1].item()):\n",
    "                    int(atac_cell_batches[i].item())]=torch.sort(\n",
    "       indices[int(atac_cell_batches[i-1].item()):\n",
    "               int(atac_cell_batches[i].item())])\n",
    "    \n",
    "   a,b,rep_r=torch.unique(\n",
    "       sorted,return_inverse =True,return_counts=True)\n",
    "   regions=torch.cat((regions,a),0)\n",
    "   region_batching[i]=a.shape[0]+region_batching[i-1] ##list of regiosn per batch\n",
    "   region_rep=torch.cat((region_rep,rep_r),0) ##how many times presented region in a batch are repeated\n",
    "   rep_r=rep_r.type(torch.LongTensor) \n",
    "   ren=torch.arange(a.shape[0]) \n",
    "   rep_r_=torch.repeat_interleave(ren, rep_r,dim=0)\n",
    "   region_rep_=torch.cat((region_rep_,rep_r_),0) \n",
    "\n",
    " region_rep=region_rep.type(torch.LongTensor) \n",
    " regions=regions.type(torch.LongTensor) \n",
    " indices=indices.int()\n",
    " indices2=indices2.int()\n",
    " region_rep_=region_rep_.type(torch.int64)\n",
    " return regions,region_batching,region_rep,region_rep_,indices,indices2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffa1ad9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization(n_cells,G,n_topics,alpha,beta,gamma,tau,device):\n",
    "    m = torch.distributions.dirichlet.Dirichlet(alpha[0,:]) \n",
    "    theta_tmp = m.sample([n_cells])\n",
    "    theta_tmp = theta_tmp.to(device)\n",
    "    \n",
    "    m = torch.distributions.gamma.Gamma(gamma,tau)\n",
    "    lam_tmp = m.sample([n_topics,G])\n",
    "    lam_tmp = lam_tmp.to(device)\n",
    "    \n",
    "    m =  torch.distributions.dirichlet.Dirichlet(beta[0,:])\n",
    "    phi_tmp = m.sample([n_topics])\n",
    "    phi_tmp = phi_tmp.to(device)\n",
    "    \n",
    "    return theta_tmp, lam_tmp, phi_tmp\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42bf59d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_to_GPU(c,region_rep, regions, indices, indices2, rep_c, rep_c_, region_rep_, R\n",
    "                , n_topics, device):\n",
    "    \n",
    "    c=c.to(device)\n",
    "    region_rep = region_rep.to(device)\n",
    "    regions = regions.to(device)\n",
    "    indices = indices.to(device)\n",
    "    indices2 = indices2.to(device)\n",
    "    rep_c = rep_c.to(device)\n",
    "    rep_c_ = rep_c_.to(device)\n",
    "    region_rep_ = region_rep_.to(device)\n",
    "    t = torch.arange(0,n_topics,dtype=torch.uint8,device=device).reshape(n_topics,1)\n",
    "    alpha = torch.ones([1,n_topics,],device=device)*(50/n_topics)\n",
    "    beta = torch.ones([1,R],device=device)*0.1\n",
    "    \n",
    "    return c, region_rep, regions, indices, indices2, rep_c, rep_c_, region_rep_, t, alpha, beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f62f514",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gibbs_Sampler(atac,rna, n_samples,n_burnin, n_b_samples, n_cells,R, G, n_topics,alpha, beta, gamma, tau, c, \n",
    "                  region_rep, regions,region_batching, indices, indices2, rep_c, rep_c_, region_rep_, t\n",
    "                  ,atac_cell_batches , theta_tmp, phi_tmp, lam_tmp,device):\n",
    "    \n",
    "    \"\"\"\n",
    "    Gibbs Sampler for a generative model with ATAC-seq and RNA-seq data.\n",
    "\n",
    "    Parameters:\n",
    "    - atac: ATAC-seq data\n",
    "    - rna: RNA-seq data\n",
    "    - n_samples: Total number of Gibbs sampling iterations\n",
    "    - n_burnin: Number of burn-in iterations\n",
    "    - n_b_samples: Number of samples to be collected after burn-in\n",
    "    - n_cells: Number of cells in the dataset\n",
    "    - R: Number of ATAC regions\n",
    "    - G: Number of genes\n",
    "    - n_topics: Number of topics in the model\n",
    "    - alpha: Dirichlet hyperparameter for theta (cell-topic distribution)\n",
    "    - beta: Dirichlet hyperparameter for phi (topic-region distribution)\n",
    "    - gamma: Gamma distribution hyperparameter for lambda (topic-gene distribution)\n",
    "    - tau: Gamma distribution hyperparameter for lambda\n",
    "    - c: Vector of cumulative cell counts for batching\n",
    "    - region_rep: Vector of region repetitions for batching\n",
    "    - regions: List of ATAC regions\n",
    "    - region_batching: Batching information for ATAC regions\n",
    "    - indices: Index information for reordering regions\n",
    "    - indices2: Second set of indices for reordering regions\n",
    "    - rep_c: Repetitions for cells in batches\n",
    "    - rep_c_: Repetitions for cells modulo batch size\n",
    "    - region_rep_: Region repetitions for cells modulo batch size\n",
    "    - t: Vector for testing conditions\n",
    "    - atac_cell_batches: Batching information for ATAC cells\n",
    "    - theta_tmp: Temporary variable for cell-topic distribution\n",
    "    - phi_tmp: Temporary variable for topic-region distribution\n",
    "    - lam_tmp: Temporary variable for topic-gene distribution\n",
    "    - device: Device to run computations on (e.g., 'cpu' or 'cuda')\n",
    "\n",
    "    Returns:\n",
    "    - theta: Cell-topic distribution samples\n",
    "    - lam: Topic-gene distribution samples\n",
    "    - phi: Topic-region distribution samples\n",
    "    \"\"\"\n",
    "    theta = torch.zeros([n_b_samples,n_cells,n_topics])\n",
    "    lam = torch.zeros([n_b_samples,n_topics,G])\n",
    "    phi = torch.zeros([n_b_samples,n_topics,R])\n",
    "    L_g = torch.zeros(n_b_samples,device=device)\n",
    "    L_r = torch.zeros(n_b_samples,device=device)\n",
    "    with tqdm(total=n_samples, desc=\"Processing\") as pbar:\n",
    "     for sample in range(0,n_samples):\n",
    "      \n",
    "      n_t_g_shape = torch.zeros([G,n_topics],device=device)\n",
    "      n_t_g_scale = torch.zeros([G,n_topics],device=device)\n",
    "      n_t_c = torch.zeros([n_cells,n_topics],device=device)\n",
    "      n_t_r = torch.zeros([R,n_topics],device=device)\n",
    "      L_g[int(sample/n_burnin)] = 0\n",
    "      L_r[int(sample/n_burnin)] = 0\n",
    "     \n",
    "      for i in torch.arange(1,c.shape[0]):\n",
    "    ########################################RNA-BATCH###################################\n",
    "    \n",
    "       rna_batch = rna[c[i-1]:c[i],:].to(device)\n",
    "       m = torch.distributions.poisson.Poisson(lam_tmp.reshape([n_topics,1,G]))\n",
    "       z_ = torch.mul(torch.exp(m.log_prob(rna_batch)),(theta_tmp[c[i-1]:c[i],:].T)[:,:,None])\n",
    "       z_L_g = torch.sum(z_,axis=0)\n",
    "       z_L_g = torch.sum(torch.log(z_L_g))\n",
    "       L_g[int(sample/n_burnin)]+= z_L_g\n",
    "       z_ = torch.div(z_,torch.sum(z_,axis=0))\n",
    "       z_[z_ != z_] = 0\n",
    "       k = torch.nonzero(torch.sum(z_,axis=0)==0)\n",
    "        \n",
    "       del m\n",
    "      \n",
    "       for j in k:\n",
    "         \n",
    "         m = torch.distributions.poisson.Poisson(lam_tmp[:,j[1]])\n",
    "         s = m.log_prob(rna_batch[j[0],j[1]])+theta_tmp[j[0],:].view(-1, 1)\n",
    "         del m\n",
    "\n",
    "\n",
    "         z_[:,j[0],j[1]] = torch.exp(s-max(s))\n",
    "        \n",
    "         z_[:,j[0],j[1]] = z_[:,j[0],j[1]]/torch.sum(z_[:,j[0],j[1]])\n",
    "         \n",
    "     \n",
    "       z_ = torch.cumsum(z_,axis=0)\n",
    "       u = torch.rand([1,(c[i]-c[i-1]).item(),G],dtype=torch.half,device=device)\n",
    "      #z_ = torch.searchsorted(torch.swapaxes(z_, 0, 2),torch.swapaxes(u, 0, 2))\n",
    "        \n",
    "       z_c = (torch.swapaxes(z_, 0, 2)).contiguous()\n",
    "       u_c = (torch.swapaxes(u, 0, 2)).contiguous()\n",
    "       z_ = torch.searchsorted(z_c,u_c)\n",
    "    \n",
    "       t_test = torch.cat(int((c[i]-c[i-1]).item())*[t],1).T\n",
    "       z_ = torch.gt((z_==t_test[None,:,:]), 0).int()\n",
    "       n_t_c[c[i-1]:c[i],:] = torch.sum(z_,axis=0)\n",
    "       n_t_g_shape+= torch.sum(z_*rna_batch.T[:,:,None],axis=1)\n",
    "       n_t_g_scale+= torch.sum(z_,axis=1)\n",
    "       del rna_batch\n",
    "    \n",
    "    #########################################ATAC_BATCH##########################################\n",
    "    \n",
    "       theta_ = theta_tmp[c[i-1]:c[i],:]\n",
    "       phi_ = phi_tmp[:,regions[int(region_batching[i-1].item()):int(region_batching[i].item())]]\n",
    "       n_regions = phi_.shape[1]\n",
    "       z_atac_ = torch.repeat_interleave(\n",
    "        theta_, rep_c[c[i-1]:c[i]],dim=0)##cells in this batch\n",
    "       phi_ = torch.repeat_interleave(\n",
    "        phi_,region_rep[int(region_batching[i-1].item()):int(region_batching[i].item())],dim=1)##regions in this batch\n",
    "       phi_ = torch.index_select(phi_,1, indices2[int(atac_cell_batches[i-1].item()):\n",
    "                                                 int(atac_cell_batches[i].item())])##reorder the regions acording to data\n",
    "      \n",
    "       z_atac_ = torch.multiply(z_atac_.T,phi_)\n",
    "       z_L_r = torch.sum(z_atac_,axis=0)\n",
    "       z_L_r = torch.sum(torch.log(z_L_r))\n",
    "       L_r[int(sample/n_burnin)]+= z_L_r\n",
    "       z_atac_ = z_atac_/torch.sum(z_atac_,axis=0)\n",
    "     \n",
    "       z_atac_ = torch.cumsum(z_atac_,axis=0)\n",
    "       u_atac = torch.rand([1,z_atac_.shape[1]],device=device)\n",
    "      \n",
    "       z_atac_ = torch.searchsorted( z_atac_.T,u_atac.T)\n",
    "      \n",
    "       z_atac_ = torch.gt((z_atac_==t.T), 0).int()\n",
    "    \n",
    "       h = rep_c_[int(atac_cell_batches[i-1].item()):\n",
    "                int(atac_cell_batches[i].item())]%batch_size###update for the cells in this batch\n",
    "     \n",
    "       n_t_c[c[i-1]:c[i],:]+= (\n",
    "        torch_scatter.scatter(z_atac_,h,dim=0,reduce=\"sum\").reshape([int((c[i]-c[i-1]).item()),n_topics]))\n",
    "    \n",
    "       z_atac_ = torch.index_select(z_atac_,0,indices[int(atac_cell_batches[i-1].item()):\n",
    "                                                    int(atac_cell_batches[i].item())])\n",
    "    \n",
    "       h = region_rep_[int(atac_cell_batches[i-1].item()):\n",
    "                     int(atac_cell_batches[i].item())]\n",
    "     \n",
    "       n_t_r[regions[int(region_batching[i-1].item()):int(region_batching[i].item())],:]+= (\n",
    "        torch_scatter.scatter(z_atac_,h,dim=0,reduce=\"sum\").reshape([n_regions,n_topics]))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    " #######################################Sampling theta, lambda, and phi##########################\n",
    "\n",
    "      m = torch.distributions.gamma.Gamma(gamma+n_t_g_shape,(n_t_g_scale*tau+1)/tau)\n",
    "      lam_tmp=m.sample()\n",
    "      lam_tmp=lam_tmp.T\n",
    "      lam[int(sample/n_burnin),:,:]=lam_tmp \n",
    "    \n",
    "      m = torch.distributions.dirichlet.Dirichlet(beta+n_t_r.T)\n",
    "      phi_tmp = m.sample()\n",
    "      phi[int(sample/n_burnin),:,:] = phi_tmp \n",
    "      m = torch.distributions.dirichlet.Dirichlet(alpha+n_t_c)\n",
    "    \n",
    "      theta_tmp = m.sample()\n",
    "      theta[int(sample/n_burnin),:,:] = theta_tmp\n",
    "      time.sleep(0.1)\n",
    "      pbar.update(1)\n",
    "    return theta, lam, phi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ec6666f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SHARE_Topic(adata_atac,adata_exp,n_topics,alpha,beta,gamma,tau,batch_size,n_samples,n_burnin,dev= \"cuda:0\",\n",
    "                save_data=True,path=\"\"):\n",
    "    \n",
    "    rna,atac=sc_to_tensor(adata_atac,adata_exp)\n",
    "    \n",
    "    if save_data:\n",
    "         torch.save(atac, path+str(\"atac_share_topic.txt\"))\n",
    "         torch.save(rna, path+str(\"rna_share_topic.txt\"))\n",
    "    ### add path to save the files\n",
    "    rna = rna.type(torch.DoubleTensor)\n",
    "    device = torch.device(dev)\n",
    "    \n",
    "    G = rna.shape[1]\n",
    "    n_cells = rna.shape[0]\n",
    "    l = torch.unique(atac[1,:])\n",
    "    R = l.shape[0]\n",
    "    alpha = torch.ones([1,n_topics])*alpha\n",
    "    beta = torch.ones([1,R])*beta\n",
    "    n_b_samples = int(n_samples/n_burnin)\n",
    "    n_batches = n_cells+batch_size-(n_cells%batch_size)\n",
    "    \n",
    "    atac_cell_batches, c, rep_c,rep_c_, ren = create_cell_batches(atac,n_batches,batch_size,n_cells,device)\n",
    "    \n",
    "    regions, region_batching, region_rep, region_rep_, indices, indices2 = create_region_batches(atac,atac_cell_batches,ren)\n",
    "    \n",
    "    theta_tmp, lam_tmp, phi_tmp = initialization(n_cells,G,n_topics,alpha,beta,gamma,tau,device)\n",
    "    \n",
    "    if dev!=\"CPU\":\n",
    "     c, region_rep, regions, indices, indices2, rep_c, rep_c_, region_rep_, t, alpha, beta = move_to_GPU(c,\n",
    "        region_rep, regions, indices, indices2, rep_c, rep_c_, region_rep_, R, n_topics, device)\n",
    "    \n",
    "    \n",
    "    \n",
    "    theta, lam, phi = Gibbs_Sampler(atac,rna, n_samples,n_burnin, n_b_samples, n_cells,R, G, n_topics,alpha, beta, gamma, tau, c, \n",
    "                  region_rep, regions, region_batching, indices, indices2, rep_c, rep_c_, region_rep_, t\n",
    "                  ,atac_cell_batches , theta_tmp, phi_tmp, lam_tmp, device)\n",
    "    \n",
    "    return theta, lam, phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c625e265",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 3/3 [00:09<00:00,  3.26s/it]\n"
     ]
    }
   ],
   "source": [
    "rna =sc.read_h5ad('/data/nelkazwi/share-topic/lymphoma_data/rna_B_lympho.h5ad')\n",
    "atac =sc.read_h5ad(\"/data/nelkazwi/share-topic/lymphoma_data/atac_B_lympho.h5ad\")\n",
    "gamma=1\n",
    "tau=0.5\n",
    "n_topics=1\n",
    "n_samples=3\n",
    "n_burnin=1\n",
    "batch_size=500\n",
    "alpha=50/n_topics\n",
    "beta=0.1\n",
    "\n",
    "theta, lam, phi=SHARE_Topic(rna,atac,n_topics,alpha,beta,gamma,tau,batch_size,n_samples,n_burnin,dev= \"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889e9579",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
